{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('vessel_label_new.csv')\n",
    "target = df.loc[:,'Class']\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "fold_no=1\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "skfold = StratifiedKFold(n_splits=9, shuffle=True, random_state=1)\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(df, target, test_size=0.20, random_state=67,stratify=Y)\n",
    "\n",
    "\n",
    "for train_index, val_index in skfold.split(df, target):\n",
    "    model_history = []\n",
    "    fold = \"fold_\" + str(fold_no)\n",
    "    print(\"FOLD: \", fold )\n",
    "    print (\" - train -\")\n",
    "  # print(\"FOLD: \", fold_no , \"TRAIN:\", train_index, \"TEST:\", val_index, target[val_index])\n",
    "    train = df.loc[train_index,:]\n",
    "    print (train['Class'].value_counts())\n",
    "#     print(train)\n",
    "    save_train_path='C:/Users/m_ktar/Downloads/VGG16-In-Keras-master/train'\n",
    "    path_train=filter_subjects(fold, \"train\", train,save_train_path)\n",
    "  \n",
    "    \n",
    "    print (\"- valid - \")\n",
    "    val = df.loc[val_index,:]\n",
    "    print (val['Class'].value_counts())\n",
    "    print (\"\\n\")\n",
    "    save_val_path='C:/Users/m_ktar/Downloads/VGG16-In-Keras-master/val_subject'\n",
    "    path_val=filter_subjects(fold, \"valid\", val,save_val_path)\n",
    "    \n",
    "    print(path_val)\n",
    "    p_t='C:/Users/m_ktar/Downloads/VGG16-In-Keras-master/train'\n",
    "    path_train=os.path.join(p_t,'fold_'+str(fold_no),'train')\n",
    "    \n",
    "    p_v='C:/Users/m_ktar/Downloads/VGG16-In-Keras-master/val'\n",
    "    path_val=os.path.join(p_v,'fold_'+str(fold_no),'valid')\n",
    "    print(path_train,path_val)\n",
    "    test=val.loc[:,'value']\n",
    "    true_lb.append(test)\n",
    "    \n",
    "#     print(true_lb)\n",
    "    Y = train.loc[:,'Class']\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(train, Y, test_size=0.20, random_state=67,stratify=Y)\n",
    "    \n",
    "# #     print(y_tr)\n",
    "\n",
    "\n",
    "    save_x_tr='E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/train_validation/train'\n",
    "    path_x_tr=filter_subjects(fold, \"train\", x_tr,save_x_tr)\n",
    "    \n",
    "    save_x_val='E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/train_validation/val'\n",
    "    path_x_val=filter_subjects(fold, \"val\", x_val,save_x_val)\n",
    "    \n",
    "    \n",
    "    p_x_tr='E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/train_validation/train'\n",
    "    path_x_tr=os.path.join(p_x_tr,'fold_'+str(fold_no),'train')\n",
    "    \n",
    "    p_x_val='E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/train_validation/val'\n",
    "    path_x_val=os.path.join(p_x_val,'fold_'+str(fold_no),'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_subjects(fold, dataset_type, df,path):\n",
    "    source_folder='E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/train_test'\n",
    "    destination_folder=path\n",
    "  # for each of the subjects in the dataframe\n",
    "    for ind in df.index:\n",
    "        subject = df['subject_list'][ind]\n",
    "#         print(subject)\n",
    "    # for each of the files in a directory\n",
    "        for folders,_,filenames in os.walk(source_folder):  \n",
    "      # for each of the filenames \n",
    "#           print(subfolders)\n",
    "          for file in filenames:\n",
    "#             print(file)\n",
    "        # if a subject is found, clone it to a folder with its class name. \n",
    "            # prefix = file[0:file.rindex(\"_\")]\n",
    "          \n",
    "#             if subject.startswith(prefix):   \n",
    "#             print(prefix)\n",
    "            if subject.split(\"_\")[1] == file.split(\"_\")[1]:\n",
    "#               print(subject, prefix)\n",
    "              \n",
    "              source = os.path.join(folders, file)\n",
    "#               print(source)\n",
    "              destination = os.path.join(destination_folder, fold, dataset_type, df['Class'][ind])\n",
    "#             print(destination)\n",
    "              from pathlib import Path\n",
    "          # check if destination folder exists; if not create.\n",
    "              Path(destination).mkdir(parents=True, exist_ok=True)\n",
    "#                 print(source, destination)\n",
    "              import shutil\n",
    "              shutil.copy2(source, destination)\n",
    "    return destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "img_augmentation = Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        layers.RandomFlip(\"vertical\"),\n",
    "        layers.RandomContrast(factor=0.5)\n",
    "#         layers.GaussianNoise(.2, seed=None)\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    from tensorflow.keras.applications.vgg16 import VGG16\n",
    "    from keras.layers import Dropout\n",
    "    vggmodel = VGG16(weights='imagenet', include_top=True)\n",
    "#     vggmodel = VGG16(weights='imagenet', include_top=True)\n",
    "    for layers in (vggmodel.layers)[:19]:\n",
    "#     print(layers)\n",
    "        layers.trainable = False\n",
    "   \n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras import optimizers\n",
    "\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from keras.layers import Input, Flatten, Dense\n",
    "    from keras.models import Model\n",
    "    fc1 = vggmodel.layers[-3]\n",
    "    fc2 = vggmodel.layers[-2]\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "\n",
    "# Create the dropout layers\n",
    "    dropout1 = Dropout(0.70)\n",
    "    dropout2 = Dropout(0.70)\n",
    "    output_vgg16 = vggmodel.output\n",
    "\n",
    "#Add the fully-connected layers \n",
    "#     x = Flatten(name='flatten')(output_vgg16)\n",
    "#     x = Dense(2048, activation='relu', name='fc1')(x)\n",
    "#     x = dropout1(x)\n",
    "#     x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "#     x = dropout2(x)\n",
    "    \n",
    "#     x = Dense(3, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    # Reconnect the layers\n",
    "    x = dropout1(fc1.output)\n",
    "    x = fc2(x)\n",
    "    x = dropout2(x)\n",
    "#     X= vggmodel.layers[-2].output\n",
    "    predictions = Dense(3, activation=\"softmax\")(x)\n",
    "    model_final = Model(inputs = vggmodel.input, outputs = predictions)\n",
    "\n",
    "    epochs=20\n",
    "    learning_rate = 0.0001\n",
    "    decay_rate = learning_rate / epochs\n",
    "    momentum = 0.8\n",
    "    model_final.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['AUC'])\n",
    "#     model_final.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "#               loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#               metrics=['AUC'])\n",
    "#     model_final.summary()\n",
    "    return model_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    from tensorflow.keras.applications import EfficientNetB0\n",
    "    inputs = layers.Input(shape=(224, 224, 3))\n",
    "    x = img_augmentation(inputs)\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = Dense(128, activation='relu', name='fc2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    top_dropout_rate = 0.7\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout2\")(x) \n",
    "    outputs = layers.Dense(2, activation=\"softmax\", name=\"pred\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', np.unique(train.classes), train.classes)\n",
    "cw = dict(zip( np.unique(train.classes), weights))\n",
    "print(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(t_x,t_y, val_x,val_y, i, EPOCHS=100, BATCH_SIZE=16):\n",
    "    model = None\n",
    "    model = cnn_model()\n",
    "    class_weight={0:.7,1:1.89,2:2.194}\n",
    "#     t_y = to_categorical(t_y)\n",
    "#     val_y = to_categorical(val_y)\n",
    "#     print(len(t_x))\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    aug = ImageDataGenerator(\n",
    "#     rotation_range=30,\n",
    "#     zoom_range=0.15,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.15,\n",
    "#     vertical_flip=True,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "    results = model.fit(x=aug.flow(t_x,t_y,batch_size=BATCH_SIZE),validation_data=(val_x,val_y),batch_size=BATCH_SIZE,epochs=epochs,class_weight=class_weight,callbacks=[early_stopping, model_checkpoint], verbose=1)  \n",
    "#     model.load_weights(\"E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/saved_models/model_\"+str(i)+\".h5\")\n",
    "#     print(\"Val Score: \", model.evaluate(val_x, val_y))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate1(train_ds,val_ds, i, EPOCHS=20, BATCH_SIZE=32):\n",
    "    model = None\n",
    "    model = cnn_model()\n",
    "    class_weight={0:.7,1:1.5,2:2.7}\n",
    "#     t_y = to_categorical(t_y)\n",
    "#     val_y = to_categorical(val_y)\n",
    "#     print(len(t_x))\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    aug = ImageDataGenerator(\n",
    "#     rotation_range=30,\n",
    "#     zoom_range=0.15,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.15,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True)\n",
    "#     fill_mode=\"nearest\")\n",
    "    \n",
    "    results = model.fit(x=aug.flow(t_x, t_y,batch_size=BATCH_SIZE),validation_data=(val_x,val_y),epochs=EPOCHS,class_weight=class_weight,callbacks=[early_stopping, model_checkpoint], \n",
    "              verbose=1)  \n",
    "# ,validation_data=(val_x,val_y)\n",
    "#     results = model.fit(train_ds,validation_data=val_ds,epochs=epochs,class_weight=class_weight,callbacks=[early_stopping, model_checkpoint], verbose=1)  \n",
    "#     model.load_weights(\"E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/saved_models/model_\"+str(i)+\".h5\")\n",
    "#     print(\"Val Score: \", model.evaluate(val_x, val_y))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare1(path):   # prepare data augmentation configuration\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    from dynamic_sampling import DynamicSamplingImageDataGenerator, BatchSizeAdapter\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "# Prepare data-augmenting data generator\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    datagen_train = ImageDataGenerator()\n",
    "    \n",
    "    \n",
    "    img_height=224\n",
    "    img_width=224\n",
    "    batch_size=16\n",
    "    train_generator = datagen_train.flow_from_directory(\n",
    "        path,\n",
    "#         class_mode='sparse',\n",
    "        target_size=(img_height, img_width))\n",
    "\n",
    "    lev=train_generator.classes\n",
    "\n",
    "    return train_generator,lev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare2(path):   # prepare data augmentation configuration\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    img_height=224\n",
    "    img_width=224\n",
    "    batch_size=16\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        path,\n",
    "#         class_mode='sparse',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    lev=test_generator.classes\n",
    "\n",
    "    return test_generator,lev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare3(path):   # prepare data augmentation configuration\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    img_height=224\n",
    "    img_width=224\n",
    "    batch_size=1\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        path,\n",
    "#         class_mode='sparse',\n",
    "        shuffle=False,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    lev=test_generator.classes\n",
    "\n",
    "    return test_generator,lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_subject(path_test,model):\n",
    "\n",
    "    pr=[]\n",
    "\n",
    "    import glob\n",
    "    import cv2\n",
    "    img_height=224\n",
    "    img_width=224\n",
    "    for img in glob.glob(os.path.join(path_test, '*.jpg')):\n",
    "        img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "        dim = (224, 224)\n",
    "        img=cv2.resize(img, dim,interpolation = cv2.INTER_AREA)\n",
    "        img_array = tf.expand_dims(img, 0) # Create a batch\n",
    "        predictions = model.predict(img_array)\n",
    "        pred=np.argmax(predictions,axis=1)\n",
    "        pr.append(pred)\n",
    "    \n",
    "    predictions= [ item for elem in pr for item in elem]\n",
    "\n",
    "    print('Flat List : ',predictions)\n",
    "    print(len(predictions))\n",
    "    \n",
    "    from collections import Counter\n",
    "\n",
    "    def find_majority(votes):\n",
    "        vote_count = Counter(votes)\n",
    "        print(vote_count)\n",
    "        top_two = vote_count.most_common(2)\n",
    "        if len(top_two)>1 and top_two[0][1] == top_two[1][1]:\n",
    "        # It is a tie\n",
    "            return 0\n",
    "        return top_two[0][0]\n",
    "\n",
    "    result=find_majority(predictions)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_custom(alpha, gamma):\n",
    "    def binary_focal_loss(y_true, y_pred):\n",
    "        import tensorflow_addons as tfa\n",
    "        fl = tfa.losses.SigmoidFocalCrossEntropy(alpha=alpha, gamma=gamma)\n",
    "        y_true_K = K.ones_like(y_true)\n",
    "        focal_loss = fl(y_true, y_pred)\n",
    "        return focal_loss\n",
    "    return binary_focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_tverskyLoss(y_true, y_pred, smooth=1e-6):\n",
    "    \n",
    "        if y_pred.shape[-1] <= 1:\n",
    "            alpha = 0.3\n",
    "            beta = 0.7\n",
    "            gamma = 4/3 #5.\n",
    "            y_pred = tf.keras.activations.sigmoid(y_pred)\n",
    "            #y_true = y_true[:,:,:,0:1]\n",
    "        elif y_pred.shape[-1] >= 2:\n",
    "            alpha = 0.3\n",
    "            beta = 0.7\n",
    "            gamma = 4/3 #3.\n",
    "            y_pred = tf.keras.activations.softmax(y_pred, axis=-1)\n",
    "            y_true = K.squeeze(y_true, 3)\n",
    "            y_true = tf.cast(y_true, \"int32\")\n",
    "            y_true = tf.one_hot(y_true, num_class, axis=-1)\n",
    "           \n",
    "        y_true = K.cast(y_true, 'float32')\n",
    "        y_pred = K.cast(y_pred, 'float32')\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = K.flatten(y_pred)\n",
    "        targets = K.flatten(y_true)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((inputs * targets))\n",
    "        FP = K.sum(((1-targets) * inputs))\n",
    "        FN = K.sum((targets * (1-inputs)))\n",
    "       \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        \n",
    "        return 1 - Tversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_individual(model,fold_no): \n",
    "    import cv2\n",
    "    from tensorflow.keras.models import load_model\n",
    "    p_v='val_2'\n",
    "    save_dir =  'saved_models/'\n",
    "#     fold_no=1\n",
    "    all_subjects_g=[]\n",
    "    all_subjects_i=[]\n",
    "    all_subjects_p=[]\n",
    "#     for i in range(1,10):\n",
    "    path_val_g=os.path.join(p_v,'fold_'+str(fold_no),'valid','good')\n",
    "#     path_val_i=os.path.join(p_v,'fold_'+str(fold_no),'valid','intermediate')\n",
    "    path_val_p=os.path.join(p_v,'fold_'+str(fold_no),'valid','poor')\n",
    "    print(path_val_g,path_val_p)\n",
    "\n",
    "    predicted_label_g=test_subject(path_val_g,model)\n",
    "#     predicted_label_i=test_subject(path_val_i,model)\n",
    "    predicted_label_p=test_subject(path_val_p,model)\n",
    "    \n",
    "#     print(\"predicted\",predicted_label)\n",
    "    all_subjects_g.append(predicted_label_g)\n",
    "#     all_subjects_i.append(predicted_label_i)\n",
    "    all_subjects_p.append(predicted_label_p)\n",
    "    print('fold:',fold_no, all_subjects_g, all_subjects_p)\n",
    "        \n",
    "#         fold_no= fold_no + 1\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from network import VGG16\n",
    "# from keras.preprocessing import image\n",
    "# from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "# from keras.models import Model\n",
    "# from keras import optimizers \n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import cv2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "   \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import time\n",
    "\n",
    "tic = time.perf_counter()\n",
    "    \n",
    "p_x_tr='train'\n",
    "\n",
    "\n",
    "p_x_val='val'\n",
    "callbacks=[]\n",
    "weights={0:1, 1:4,2:1}\n",
    "fold_no=1\n",
    "for i in range(1,10):\n",
    "    path_x_tr=os.path.join(p_x_tr,'fold_'+str(fold_no),'train')\n",
    "    path_x_val=os.path.join(p_x_val,'fold_'+str(fold_no),'val')  \n",
    "    \n",
    "    t_x,t_y=data_prepare1(path_x_tr)\n",
    "    val_x,val_y=data_prepare2(path_x_val)\n",
    "    \n",
    "    trainX=t_x\n",
    "   \n",
    "    testX=val_x\n",
    "    testy=val_y\n",
    "    \n",
    "    datagen_train=trainX\n",
    "    datagen_valid=testX\n",
    "    \n",
    "\n",
    "\n",
    "    save_dir =  'saved_models/focal_loss_weight/'\n",
    "    checkpoint_path=save_dir+get_model_name(fold_no)\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from tensorflow.keras.models import load_model\n",
    "    \n",
    "\n",
    "    model=None\n",
    "    model=cnn_model()\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience= 7, factor= 0.5, min_lr= 1e-7, verbose=1)\n",
    "    ep=200\n",
    "    \n",
    "    from collections import Counter\n",
    "    c=Counter(t_y)\n",
    "    total=c[0]+c[1]\n",
    "    weight_for_0 = (1 / c[0]) * (total / 2.0)\n",
    "    weight_for_1 = (1 / c[1]) * (total / 2.0)\n",
    "\n",
    "    weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    \n",
    "    # Create hyperparameter space\n",
    "    epochs = [2, 4]\n",
    "    batches = [8, 16, 32, 64]\n",
    "    lr=[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6]\n",
    "    optimizers = [tf.keras.optimizers.rmsprop(learning_rate=lr), tf.keras.optimizers.Adam(learning_rate=lr)]\n",
    "    loss_f=[focal_loss_custom(alpha=0.2, gamma=2.0),focal_tverskyLoss]\n",
    "\n",
    "    from focal_loss import BinaryFocalLoss\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "    model.compile(\n",
    "        optimizer=optimizers, loss=loss_f, metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model_history = model.fit(datagen_train, validation_data =datagen_valid,batch_size=batches,callbacks=[rlrop,early_stop],class_weight=weights,verbose = 1)\n",
    "#saving the trained model weights as data file in .h5 format\n",
    "    filename=save_dir+get_model_name(fold_no)\n",
    "    # View hyperparameters of best neural network\n",
    "    #grid_result.best_params_\n",
    "   \n",
    "    print(\"Total parameters searched:\")\n",
    "    print(\"   \",hyperparameters)\n",
    "    print(\"Best parameters are:\")\n",
    "    print(\"   \",model_history.best_params_)\n",
    "    print(\"With a score of:\")\n",
    "    print(\"   \",model_history.best_score_)\n",
    "    model.save_weights(filename)\n",
    "    \n",
    "    ##########--------------new case testing--------------######\n",
    "    p_t='C:/Users/m_ktar/Downloads/VGG16-In-Keras-master/val_2'\n",
    "    path_test=os.path.join(p_t,'fold_'+str(fold_no),'valid')\n",
    "    newX,newy=data_prepare3(path_test)\n",
    "#     loss,accuracy = model.evaluate(newX)\n",
    "#     print('Test accuracy :', accuracy)\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    predIdxs = model.predict(newX)\n",
    "    predIdxs = np.argmax(predIdxs, axis=1)\n",
    "    print(classification_report(newy, predIdxs))\n",
    "    \n",
    "    check_individual(model,fold_no)\n",
    "#     filename=save_dir+get_model_name(fold_no)\n",
    "#     model.save(filename)\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"one fold is taking {toc - tic:0.4f} seconds to complete\")\n",
    "\n",
    "    fold_no= fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hist1=model_history\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"loss\"])\n",
    "#     plt.plot(hist.history[\"val_loss\"])\n",
    "#     plt.plot(hist.history[\"lr\"])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(hist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "p_v='C:/Users/m_ktar/Downloads/VGG16-In-Keras-master/val'\n",
    "save_dir =  'E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/saved_models/'\n",
    "fold_no=1\n",
    "all_subjects_g=[]\n",
    "all_subjects_i=[]\n",
    "all_subjects_p=[]\n",
    "for i in range(9):\n",
    "    path_val_g=os.path.join(p_v,'fold_'+str(fold_no),'valid','good')\n",
    "    path_val_i=os.path.join(p_v,'fold_'+str(fold_no),'valid','intermediate')\n",
    "    path_val_p=os.path.join(p_v,'fold_'+str(fold_no),'valid','poor')\n",
    "    print(path_val_g,path_val_i,path_val_p)\n",
    "        \n",
    "    model=None\n",
    "    model=cnn_model()\n",
    "    model.load_weights(save_dir+get_model_name(fold_no))\n",
    "#     model=load_model(save_dir+get_model_name(fold_no)) \n",
    "    predicted_label_g=test_subject(path_val_g,model)\n",
    "    predicted_label_i=test_subject(path_val_i,model)\n",
    "    predicted_label_p=test_subject(path_val_p,model)\n",
    "    \n",
    "#     print(\"predicted\",predicted_label)\n",
    "    all_subjects_g.append(predicted_label_g)\n",
    "    all_subjects_i.append(predicted_label_i)\n",
    "    all_subjects_p.append(predicted_label_p)\n",
    "    print('fold:',i+1, all_subjects_g, all_subjects_i,all_subjects_p)\n",
    "    fold_no= fold_no + 1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_test='E:/mumu/data_2d_from_4d_mip_filtered/test/poor'\n",
    "# class_names=test_dataset.class_names\n",
    "pr=[]\n",
    "#model=load_model('testmodel_gc.h5')\n",
    "model=cnn_model()\n",
    "model.load_weights('E:/mumu/data_2d_from_4d_mip_filtered/all_corrected/stacked_overlap_classification/saved_models/model_9.h5')\n",
    "import glob\n",
    "import cv2\n",
    "img_height=221\n",
    "img_width=221\n",
    "for img in glob.glob(\"C:/Users/m_ktar/Downloads/VGG16-In-Keras-master/val/fold_9/valid/intermediate/s_84_*.jpg\"):\n",
    "    img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    dim = (224, 224)\n",
    "    img=cv2.resize(img, dim,interpolation = cv2.INTER_AREA)\n",
    "    img_array = tf.expand_dims(img, 0) # Create a batch\n",
    "#     print(img_array.shape)\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    pred=np.argmax(predictions,axis=1)\n",
    "#     print(pred)\n",
    "    pr.append(pred)\n",
    "    \n",
    "pred= [ item for elem in pr for item in elem]\n",
    "\n",
    "print('Flat List : ',pred)\n",
    "print(len(pred))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_majority(votes):\n",
    "    vote_count = Counter(votes)\n",
    "    print(vote_count)\n",
    "    top_two = vote_count.most_common(2)\n",
    "    if len(top_two)>1 and top_two[0][1] == top_two[1][1]:\n",
    "        # It is a tie\n",
    "        return 0\n",
    "    return top_two[0][0]\n",
    "\n",
    "result=find_majority(pred)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
